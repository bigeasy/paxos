Replicating state machines is a way of distributing a fault-tolerant service to a number of clients, with each replica maintaining a state consistent with the others. Fault tolerance is achieved by having a number of replicas with the same state, allowing a number of them (maintaining 2N+1 replicas where N = number of tolerable failures) to experience any sort of failure without affecting the remaining system.

Replicas should be able to rejoin the proceedings after a failure, which means they will require an input log to repeat inputs in the correct order and reach the correct state (determinism assumed, of course), or a recent known state held by the cluster and the inputs since that state. It is assumed that all replicas should keep some sort of log; the system is not very fault-tolerant if all of them do not. In terms of paxos, it makes sense for all nodes except clients and, in some situations, learners, to log their recent state and inputs. Otherwise, clients and learners are solely interested in outputs.

Paxos is a protocol that allows a system of fault-tolerant machines to reach a consensus. A leader is required when two or more machines cannot reach an agreement on the next operation, such as when two proposers are continually racing each other. Thus, a Paxos implementation requires *some* algorithm that will always eventually decide on a leader. These things also allow Paxos to continue working when a number of machines have failed; as long as a leader is chosen among the remaining, working machines, inputs will continue to be processed and failed machines can join at a later time. Although only one machine is actually required to continue processing inputs (assuming it can fill the roles of both proposer and acceptor, an option all nodes should have), although fault tolerance will of course not be restored until at least three machines are running.

There are a number of small optimizations that can improve the speed of Paxos rounds in different situations. For example, if reaching consensus is expected to take a while but network latency is not an issue, that round can be sped up by sending denial responses to failed promise requests instead of ignoring them. There are also a number of ways to distinguish groups of nodes that can make each node's job simpler. All learners, for example, could be grouped together with a single learner determined leader. Thus, all acceptors only need to send accepted values to one learner. As long as there is an algorithm in place to choose a new leader for the cluster of learners and notify the acceptors, rounds can continue as soon as that leader has stored the output of the last round and the other learners can be updated as the next round continues.
